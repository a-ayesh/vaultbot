{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📄 Data Preparation Notebook\n",
    "\n",
    "**Purpose:** Prepare and convert raw product data (Excel/CSV/JSON) into clean, RAG-ready Markdown for downstream pipelines.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup & Configuration](#setup)\n",
    "2. [JSON to Markdown](#json2md)\n",
    "3. [XLSX to CSV (per sheet)](#xlsx2csv)\n",
    "4. [CSV to Markdown (Q&A)](#csv2md)\n",
    "5. [Rate Sheet Markdown](#ratesheet)\n",
    "6. [Product List Markdown](#productlist)\n",
    "7. [Summary & Next Steps](#summary)\n",
    "\n",
    "---\n",
    "\n",
    "**Authors:**\n",
    "- Ayesh Ahmad (365966)\n",
    "- Farooq Afzal (365793)\n",
    "- Muhammad Faras Siddiqui (365988)\n",
    "\n",
    "**Last Updated:** May 4, 2025\n",
    "\n",
    "**Version:** 1.0\n",
    "\n",
    "**Environment:** Python 3.12.10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration <a id='setup'></a>\n",
    "---\n",
    "This section sets up necessary imports, paths, and configuration for the data processing pipeline.\n",
    "\n",
    "### 1.1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Union, Tuple, Any\n",
    "from collections import defaultdict\n",
    "\n",
    "# Third party imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "logger = logging.getLogger('data_preparation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Directory Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-04 15:43:18 - data_preparation - INFO - Directory ensured: data/raw\n",
      "2025-05-04 15:43:18 - data_preparation - INFO - Directory ensured: data/processing\n",
      "2025-05-04 15:43:18 - data_preparation - INFO - Directory ensured: data/processed\n",
      "2025-05-04 15:43:18 - data_preparation - INFO - Directory ensured: data/processing/mds\n",
      "2025-05-04 15:43:18 - data_preparation - INFO - Directory ensured: data/processing/csvs\n"
     ]
    }
   ],
   "source": [
    "# Define directory structure\n",
    "RAW_DIR = Path('./data/raw')\n",
    "PROC_DIR = Path('./data/processing')\n",
    "PROCESSED_DIR = Path('./data/processed')\n",
    "MD_DIR = PROC_DIR / 'mds'\n",
    "CSV_DIR = PROC_DIR / 'csvs'\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for directory in [RAW_DIR, PROC_DIR, PROCESSED_DIR, MD_DIR, CSV_DIR]:\n",
    "    directory.mkdir(parents=True, exist_ok=True)\n",
    "    logger.info(f\"Directory ensured: {directory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Constants and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Constants\n",
    "CONFIG = {\n",
    "    \"file_paths\": {\n",
    "        \"faq_json\": RAW_DIR / 'funds_transfer_app_features_faq.json',\n",
    "        \"products_xlsx\": RAW_DIR / 'NUST Bank-Product-Knowledge.xlsx',\n",
    "        \"rate_sheet_csv\": CSV_DIR / 'Rate_Sheet_July_1_2024.csv',\n",
    "    },\n",
    "    \"output_paths\": {\n",
    "        \"faq_md\": MD_DIR / 'faq.md',\n",
    "        \"rate_sheet_md\": MD_DIR / 'Rate_Sheet.md',\n",
    "        \"combined_md\": PROC_DIR / 'unreviewed_RAG.md',\n",
    "    },\n",
    "    \"excluded_csvs\": ['Rate_Sheet_July_1_2024.csv', 'Main.csv', 'Sheet1.csv']\n",
    "}\n",
    "\n",
    "# List of recognized savings accounts for rate sheet processing\n",
    "KNOWN_SAVINGS_ACCOUNTS = [\n",
    "    \"NUST Asaan Account\",\n",
    "    \"PLS Savings\",\n",
    "    \"Little Champs Account\", \n",
    "    \"NUST Special Deposit Account (ASDA)\",\n",
    "    \"NUST Waqaar Account - Senior Citizen\",\n",
    "    \"PakWatan Remittance Account\",\n",
    "    \"NUST Sahar Savings Account\",\n",
    "    \"NUST Maximiser Savings Account\",\n",
    "    \"PLS Pensioners Account\"\n",
    "]\n",
    "\n",
    "# Product mapping for final combined markdown\n",
    "PRODUCTS = {\n",
    "    'Services and Liability': [\n",
    "        ('NAA.md', 'NUST Asaan Account'),\n",
    "        ('CDA.md', 'Current Deposit Account'),\n",
    "        ('VPCA.md', 'Value Plus Current Account'),\n",
    "        ('VP-BA.md', 'Value Plus Business Account'),\n",
    "        ('VPBA.md', 'Value Premium Business Account'),\n",
    "        ('NSDA.md', 'NUST Special Deposit Account'),\n",
    "        ('PLS.md', 'Profit And Loss Sharing Account'),\n",
    "        ('NFDA.md', 'NUST Freelancer Deposit Account'),\n",
    "        ('NMA.md', 'NUST Maximiser Account'),\n",
    "        ('NADRA.md', 'NUST Asaan Digital Remittance Account'),\n",
    "        ('NSA.md', 'NUST Sahar Account'),\n",
    "        ('NADA.md', 'NUST Asaan Digital Account'),\n",
    "        ('PWRA.md', 'PakWatan Remittance Account'),\n",
    "        ('RDA.md', 'Roshan Digital Account'),\n",
    "        ('LCA.md', 'Little Champs Account'),\n",
    "        ('NWA.md', 'NUST Waqaar Account'),\n",
    "        ('HOME_REMITTANCE.md', 'NUST Home Remittance'),\n",
    "    ],\n",
    "    'Consumer': [\n",
    "        ('NMC.md', 'NUST MasterCard'),\n",
    "        ('NUST4Car.md', 'NUST4Car'),\n",
    "        ('NMF.md', 'NUST Mortgage Finance'),\n",
    "        ('PF.md', 'Personal Finance'),\n",
    "    ],\n",
    "    'SME': [\n",
    "        ('NUF.md', 'NUST Ujala Finance'),\n",
    "        ('NSF.md', 'NUST Sahar Finance'),\n",
    "        ('NFBF.md', 'NUST Fauri Business Finance'),\n",
    "        ('PMYB__ALS.md', 'Prime Minister Youth Business and Agriculture Loan Scheme'),\n",
    "        ('NHF.md', 'NUST Hunarmand Finance'),\n",
    "        ('NFMF.md', 'NUST Flour Mill Finance'),\n",
    "        ('NIF.md', 'NUST Imarat Finance'),\n",
    "        ('NRF.md', 'NUST Rice Finance'),\n",
    "    ],\n",
    "    'Third Party': [\n",
    "        ('NUST_Life.md', 'NUST Life Bancassurance Policy'),\n",
    "        ('EFU_Life.md', 'EFU Life Bancassurance Policy'),\n",
    "        ('Jubilee_Life_.md', 'Jubilee Life Bancassurance Policy'),\n",
    "    ],\n",
    "    'Other': [\n",
    "        ('ESFCA.md', 'Exporters Special Foreign Currency Account'),\n",
    "        ('Rate_Sheet.md', 'Rate Sheet'),\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_markdown(markdown_text: str, output_path: Path) -> None:\n",
    "    \"\"\"Save Markdown content to a file.\n",
    "    \n",
    "    Args:\n",
    "        markdown_text: The markdown content to save\n",
    "        output_path: Path to save the markdown file\n",
    "    \"\"\"\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(markdown_text)\n",
    "    \n",
    "    logger.info(f\"Saved markdown file: {output_path}\")\n",
    "\n",
    "def read_file(file_path: Path) -> str:\n",
    "    \"\"\"Read file content if it exists.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the file to read\n",
    "        \n",
    "    Returns:\n",
    "        str: File content as string, empty string if file not found\n",
    "    \"\"\"\n",
    "    if file_path.exists():\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "    \n",
    "    logger.warning(f\"File not found: {file_path.name}\")\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. JSON to Markdown <a id='json2md'></a>\n",
    "---\n",
    "This section handles the conversion of structured FAQ JSON data to a more readable and RAG-friendly Markdown format.\n",
    "\n",
    "### 2.1 JSON Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(file_path: Path) -> dict:\n",
    "    \"\"\"Load JSON data from a file.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the JSON file\n",
    "        \n",
    "    Returns:\n",
    "        dict: Loaded JSON data as dictionary\n",
    "        \n",
    "    Raises:\n",
    "        FileNotFoundError: If the file doesn't exist\n",
    "        json.JSONDecodeError: If the file contains invalid JSON\n",
    "    \"\"\"\n",
    "    logger.info(f\"Loading JSON from: {file_path}\")\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"JSON file not found: {file_path}\")\n",
    "        raise\n",
    "    except json.JSONDecodeError as e:\n",
    "        logger.error(f\"Invalid JSON format in {file_path}: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def json_to_faq_markdown(data: dict) -> str:\n",
    "    \"\"\"Convert structured FAQ JSON to RAG-friendly Markdown.\n",
    "    \n",
    "    Expected JSON structure:\n",
    "    {\n",
    "        \"categories\": [\n",
    "            {\n",
    "                \"category\": \"Category Name\",\n",
    "                \"questions\": [\n",
    "                    {\"question\": \"Question text?\", \"answer\": \"Answer text\"}\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    Args:\n",
    "        data: JSON data as a dictionary\n",
    "        \n",
    "    Returns:\n",
    "        str: Markdown formatted FAQ content\n",
    "    \"\"\"\n",
    "    logger.info(\"Converting JSON data to FAQ markdown\")\n",
    "    markdown = []\n",
    "    \n",
    "    # Track statistics for logging\n",
    "    category_count = 0\n",
    "    question_count = 0\n",
    "    \n",
    "    for category in data.get('categories', []):\n",
    "        cat_title = category.get('category', 'General')\n",
    "        category_count += 1\n",
    "        \n",
    "        markdown.append(f'\\n## FAQ: {cat_title}\\n')\n",
    "        \n",
    "        for qa in category.get('questions', []):\n",
    "            question = qa.get('question', '').strip()\n",
    "            answer = qa.get('answer', '').strip()\n",
    "            \n",
    "            if question and answer:\n",
    "                markdown.append(f'\\n#### Q: {question}\\n**A:**  \\n{answer}\\n')\n",
    "                question_count += 1\n",
    "    \n",
    "    logger.info(f\"Converted {category_count} categories with {question_count} Q&A pairs\")\n",
    "    return '\\n'.join(markdown).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Process FAQ JSON to Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_faq_json():\n",
    "    \"\"\"Process FAQ JSON file to markdown.\n",
    "    \n",
    "    Uses the configuration defined in CONFIG dictionary.\n",
    "    \"\"\"\n",
    "    # Get file paths from config\n",
    "    json_input_path = CONFIG[\"file_paths\"][\"faq_json\"]\n",
    "    json_output_path = CONFIG[\"output_paths\"][\"faq_md\"]\n",
    "    \n",
    "    logger.info(f\"Processing FAQ JSON: {json_input_path} → {json_output_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Load and process JSON data\n",
    "        json_data = load_json(json_input_path)\n",
    "        md_text = json_to_faq_markdown(json_data)\n",
    "        \n",
    "        # Save the resulting markdown\n",
    "        save_markdown(md_text, json_output_path)\n",
    "        logger.info(f\"Successfully converted FAQ JSON to markdown\")\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing FAQ JSON: {str(e)}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-04 15:44:44 - data_preparation - INFO - Processing FAQ JSON: data/raw/funds_transfer_app_features_faq.json → data/processing/mds/faq.md\n",
      "2025-05-04 15:44:44 - data_preparation - INFO - Loading JSON from: data/raw/funds_transfer_app_features_faq.json\n",
      "2025-05-04 15:44:44 - data_preparation - INFO - Converting JSON data to FAQ markdown\n",
      "2025-05-04 15:44:44 - data_preparation - INFO - Converted 2 categories with 15 Q&A pairs\n",
      "2025-05-04 15:44:44 - data_preparation - INFO - Saved markdown file: data/processing/mds/faq.md\n",
      "2025-05-04 15:44:44 - data_preparation - INFO - Successfully converted FAQ JSON to markdown\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[✓] Saved Markdown from JSON: data/processing/mds/faq.md\n"
     ]
    }
   ],
   "source": [
    "# Run the FAQ JSON conversion\n",
    "process_faq_json()\n",
    "print(f'[✓] Saved Markdown from JSON: {CONFIG[\"output_paths\"][\"faq_md\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. XLSX to CSV (per sheet) <a id='xlsx2csv'></a>\n",
    "---\n",
    "This section handles the conversion of Excel workbook sheets into individual CSV files for further processing.\n",
    "\n",
    "### 3.1 XLSX to CSV Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_xlsx_to_csvs(xlsx_file_path: Path, output_dir: Path = None) -> List[Path]:\n",
    "    \"\"\"Convert each sheet in an XLSX file to individual CSVs.\n",
    "    \n",
    "    Args:\n",
    "        xlsx_file_path: Path to the Excel file\n",
    "        output_dir: Directory to save CSV files in (default: dir with file name)\n",
    "        \n",
    "    Returns:\n",
    "        List[Path]: List of paths to the generated CSV files\n",
    "        \n",
    "    Raises:\n",
    "        FileNotFoundError: If the Excel file doesn't exist\n",
    "        ValueError: If the Excel file is invalid or empty\n",
    "    \"\"\"\n",
    "    logger.info(f\"Converting Excel file to CSVs: {xlsx_file_path}\")\n",
    "    \n",
    "    if not xlsx_file_path.exists():\n",
    "        logger.error(f\"Excel file not found: {xlsx_file_path}\")\n",
    "        raise FileNotFoundError(f\"Excel file not found: {xlsx_file_path}\")\n",
    "    \n",
    "    # Set up output directory\n",
    "    output_dir = output_dir or xlsx_file_path.with_suffix('')\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Open Excel file\n",
    "    try:\n",
    "        excel_file = pd.ExcelFile(xlsx_file_path)\n",
    "        sheet_names = excel_file.sheet_names\n",
    "        \n",
    "        if not sheet_names:\n",
    "            logger.warning(f\"No sheets found in Excel file: {xlsx_file_path}\")\n",
    "            return []\n",
    "            \n",
    "        logger.info(f\"Found {len(sheet_names)} sheets in {xlsx_file_path.name}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error opening Excel file: {str(e)}\")\n",
    "        raise ValueError(f\"Invalid Excel file: {str(e)}\")\n",
    "    \n",
    "    # Process each sheet\n",
    "    csv_files = []\n",
    "    \n",
    "    for sheet_name in sheet_names:\n",
    "        try:\n",
    "            df = excel_file.parse(sheet_name)\n",
    "            \n",
    "            # Create a safe filename from sheet name\n",
    "            safe_name = re.sub(r'[\\s/&]', '_', sheet_name)\n",
    "            csv_file_path = output_dir / f'{safe_name}.csv'\n",
    "            \n",
    "            # Save as CSV\n",
    "            df.to_csv(csv_file_path, index=False)\n",
    "            csv_files.append(csv_file_path)\n",
    "            \n",
    "            logger.info(f\"Exported sheet '{sheet_name}' to {csv_file_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing sheet '{sheet_name}': {str(e)}\")\n",
    "    \n",
    "    return csv_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Process Excel to CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-04 15:45:13 - data_preparation - INFO - Converting Excel file to CSVs: data/raw/NUST Bank-Product-Knowledge.xlsx\n",
      "2025-05-04 15:45:15 - data_preparation - INFO - Found 36 sheets in NUST Bank-Product-Knowledge.xlsx\n",
      "2025-05-04 15:45:15 - data_preparation - INFO - Exported sheet 'Main' to data/processing/csvs/Main.csv\n",
      "2025-05-04 15:45:15 - data_preparation - INFO - Exported sheet 'Rate Sheet July 1 2024' to data/processing/csvs/Rate_Sheet_July_1_2024.csv\n",
      "2025-05-04 15:45:15 - data_preparation - INFO - Exported sheet 'LCA' to data/processing/csvs/LCA.csv\n",
      "2025-05-04 15:45:15 - data_preparation - INFO - Exported sheet 'NAA' to data/processing/csvs/NAA.csv\n",
      "2025-05-04 15:45:15 - data_preparation - INFO - Exported sheet 'NWA' to data/processing/csvs/NWA.csv\n",
      "2025-05-04 15:45:15 - data_preparation - INFO - Exported sheet 'PWRA' to data/processing/csvs/PWRA.csv\n",
      "2025-05-04 15:45:15 - data_preparation - INFO - Exported sheet 'RDA' to data/processing/csvs/RDA.csv\n",
      "2025-05-04 15:45:15 - data_preparation - INFO - Exported sheet 'VPCA' to data/processing/csvs/VPCA.csv\n",
      "2025-05-04 15:45:15 - data_preparation - INFO - Exported sheet 'VP-BA' to data/processing/csvs/VP-BA.csv\n",
      "2025-05-04 15:45:15 - data_preparation - INFO - Exported sheet 'VPBA' to data/processing/csvs/VPBA.csv\n",
      "2025-05-04 15:45:15 - data_preparation - INFO - Exported sheet 'NSDA' to data/processing/csvs/NSDA.csv\n",
      "2025-05-04 15:45:15 - data_preparation - INFO - Exported sheet 'PLS' to data/processing/csvs/PLS.csv\n",
      "2025-05-04 15:45:15 - data_preparation - INFO - Exported sheet 'CDA' to data/processing/csvs/CDA.csv\n",
      "2025-05-04 15:45:15 - data_preparation - INFO - Exported sheet 'NMA' to data/processing/csvs/NMA.csv\n",
      "2025-05-04 15:45:15 - data_preparation - INFO - Exported sheet 'NADA' to data/processing/csvs/NADA.csv\n",
      "2025-05-04 15:45:15 - data_preparation - INFO - Exported sheet 'NADRA' to data/processing/csvs/NADRA.csv\n",
      "2025-05-04 15:45:16 - data_preparation - INFO - Exported sheet 'NUST4Car' to data/processing/csvs/NUST4Car.csv\n",
      "2025-05-04 15:45:16 - data_preparation - INFO - Exported sheet 'ESFCA' to data/processing/csvs/ESFCA.csv\n",
      "2025-05-04 15:45:16 - data_preparation - INFO - Exported sheet 'NFDA' to data/processing/csvs/NFDA.csv\n",
      "2025-05-04 15:45:16 - data_preparation - INFO - Exported sheet 'NSA' to data/processing/csvs/NSA.csv\n",
      "2025-05-04 15:45:16 - data_preparation - INFO - Exported sheet 'PF' to data/processing/csvs/PF.csv\n",
      "2025-05-04 15:45:16 - data_preparation - INFO - Exported sheet 'NMC' to data/processing/csvs/NMC.csv\n",
      "2025-05-04 15:45:16 - data_preparation - INFO - Exported sheet 'NMF' to data/processing/csvs/NMF.csv\n",
      "2025-05-04 15:45:16 - data_preparation - INFO - Exported sheet 'NSF' to data/processing/csvs/NSF.csv\n",
      "2025-05-04 15:45:16 - data_preparation - INFO - Exported sheet 'NIF' to data/processing/csvs/NIF.csv\n",
      "2025-05-04 15:45:16 - data_preparation - INFO - Exported sheet 'NUF' to data/processing/csvs/NUF.csv\n",
      "2025-05-04 15:45:16 - data_preparation - INFO - Exported sheet 'NFMF' to data/processing/csvs/NFMF.csv\n",
      "2025-05-04 15:45:16 - data_preparation - INFO - Exported sheet 'NFBF' to data/processing/csvs/NFBF.csv\n",
      "2025-05-04 15:45:16 - data_preparation - INFO - Exported sheet 'PMYB &ALS' to data/processing/csvs/PMYB__ALS.csv\n",
      "2025-05-04 15:45:16 - data_preparation - INFO - Exported sheet 'NRF' to data/processing/csvs/NRF.csv\n",
      "2025-05-04 15:45:16 - data_preparation - INFO - Exported sheet 'NHF' to data/processing/csvs/NHF.csv\n",
      "2025-05-04 15:45:16 - data_preparation - INFO - Exported sheet 'Nust Life' to data/processing/csvs/Nust_Life.csv\n",
      "2025-05-04 15:45:16 - data_preparation - INFO - Exported sheet 'EFU Life' to data/processing/csvs/EFU_Life.csv\n",
      "2025-05-04 15:45:16 - data_preparation - INFO - Exported sheet 'Jubilee Life ' to data/processing/csvs/Jubilee_Life_.csv\n",
      "2025-05-04 15:45:16 - data_preparation - INFO - Exported sheet 'HOME REMITTANCE' to data/processing/csvs/HOME_REMITTANCE.csv\n",
      "2025-05-04 15:45:16 - data_preparation - INFO - Exported sheet 'Sheet1' to data/processing/csvs/Sheet1.csv\n",
      "2025-05-04 15:45:16 - data_preparation - INFO - Successfully converted 36 sheets to CSV format\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[✓] Exported 36 sheets to CSV format\n"
     ]
    }
   ],
   "source": [
    "def process_xlsx_to_csvs():\n",
    "    \"\"\"Process XLSX file to CSV files.\n",
    "    \n",
    "    Uses the configuration defined in CONFIG dictionary.\n",
    "    \"\"\"\n",
    "    # Get file paths from config\n",
    "    xlsx_input = CONFIG[\"file_paths\"][\"products_xlsx\"]\n",
    "    \n",
    "    try:\n",
    "        # Convert XLSX to CSVs\n",
    "        csv_files = convert_xlsx_to_csvs(xlsx_input, CSV_DIR)\n",
    "        logger.info(f\"Successfully converted {len(csv_files)} sheets to CSV format\")\n",
    "        return len(csv_files)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing Excel file: {str(e)}\")\n",
    "        return 0\n",
    "\n",
    "# Run the Excel to CSV conversion\n",
    "num_csvs = process_xlsx_to_csvs()\n",
    "print(f'[✓] Exported {num_csvs} sheets to CSV format')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. CSV to Markdown (Q&A) <a id='csv2md'></a>\n",
    "---\n",
    "This section handles the conversion of Q&A CSVs to structured markdown documents using custom rules and heuristics.\n",
    "\n",
    "### 4.1 Q&A Text Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_question(text: str) -> bool:\n",
    "    \"\"\"Determine if a given text is likely a question.\n",
    "    \n",
    "    Uses heuristics like question marks and question-starting words.\n",
    "    \n",
    "    Args:\n",
    "        text: The text to evaluate\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if text is likely a question\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return False\n",
    "        \n",
    "    question_words = (\n",
    "        'what', 'how', 'why', 'when', 'where', 'who', 'which', \n",
    "        'can', 'is', 'are', 'do', 'does', 'will', 'would', 'should', 'could'\n",
    "    )\n",
    "    text_lower = text.lower().strip()\n",
    "    \n",
    "    return '?' in text or any(text_lower.startswith(word) for word in question_words)\n",
    "\n",
    "def merge_short_value_to_previous(lines: list) -> list:\n",
    "    \"\"\"Merge short numeric/text values with previous lines.\n",
    "    \n",
    "    Helps with formatting answers where values like rates or numbers\n",
    "    are split onto their own lines but should be part of previous text.\n",
    "    \n",
    "    Args:\n",
    "        lines: List of text lines to process\n",
    "        \n",
    "    Returns:\n",
    "        list: List with short values merged with previous lines\n",
    "    \"\"\"\n",
    "    merged = []\n",
    "    \n",
    "    for line in lines:\n",
    "        # If line is short, alphanumeric, and we have previous content\n",
    "        if (len(str(line).split()) <= 2 and \n",
    "            re.match(r'^[\\d.,a-zA-Z\\s]+$', str(line)) and \n",
    "            merged):\n",
    "            merged[-1] += f' {line}'\n",
    "        else:\n",
    "            merged.append(str(line))\n",
    "            \n",
    "    return merged\n",
    "\n",
    "def format_answer(lines: list) -> str:\n",
    "    \"\"\"Format a list of answer lines into markdown.\n",
    "    \n",
    "    Handles bullet points, formatting, and merges short values.\n",
    "    \n",
    "    Args:\n",
    "        lines: List of answer text lines\n",
    "        \n",
    "    Returns:\n",
    "        str: Formatted markdown for the answer\n",
    "    \"\"\"\n",
    "    # Merge short numeric values with previous lines\n",
    "    lines = merge_short_value_to_previous(lines)\n",
    "    \n",
    "    formatted_lines = []\n",
    "    for line in lines:\n",
    "        clean = str(line).lstrip('o ').strip()\n",
    "        \n",
    "        # Handle bullet points\n",
    "        if (line.startswith(('o ', 'o', '*', '-')) or \n",
    "            clean.lower().startswith('free') or \n",
    "            clean.endswith(':')):\n",
    "            formatted_lines.append(f'* {clean}')\n",
    "        else:\n",
    "            formatted_lines.append(clean)\n",
    "            \n",
    "    return '\\n'.join(formatted_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 CSV Content Extraction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_blocks(df: pd.DataFrame) -> list:\n",
    "    \"\"\"Extract Q&A blocks from a dataframe.\n",
    "    \n",
    "    Scans through the dataframe to identify question-answer pairs\n",
    "    based on heuristics like question structure.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing Q&A content\n",
    "        \n",
    "    Returns:\n",
    "        list: List of (question, answer_lines) tuples\n",
    "    \"\"\"\n",
    "    # Drop completely empty rows\n",
    "    df = df.dropna(how='all')\n",
    "    blocks = []\n",
    "    \n",
    "    current_question = None\n",
    "    current_answer = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        # Get non-empty cells from the row\n",
    "        cells = [str(cell).strip() for cell in row if pd.notna(cell) and str(cell).strip()]\n",
    "        \n",
    "        for line in cells:\n",
    "            if is_question(line):\n",
    "                # If we have a previous question, add it to blocks\n",
    "                if current_question:\n",
    "                    blocks.append((current_question, current_answer))\n",
    "                    \n",
    "                # Start new question\n",
    "                current_question = line\n",
    "                current_answer = []\n",
    "            elif current_question:\n",
    "                # Add line to current answer\n",
    "                current_answer.append(line)\n",
    "    \n",
    "    # Add the last question if any\n",
    "    if current_question:\n",
    "        blocks.append((current_question, current_answer))\n",
    "        \n",
    "    return blocks\n",
    "\n",
    "def convert_blocks_to_markdown(blocks: list) -> str:\n",
    "    \"\"\"Convert Q&A blocks to markdown format.\n",
    "    \n",
    "    Args:\n",
    "        blocks: List of (question, answer_lines) tuples\n",
    "        \n",
    "    Returns:\n",
    "        str: Markdown formatted Q&A content\n",
    "    \"\"\"\n",
    "    markdown_blocks = []\n",
    "    \n",
    "    for question, answer_lines in blocks:\n",
    "        markdown = f'#### Q: {question.strip()}\\n**A:**\\n'\n",
    "        formatted_answer = format_answer(answer_lines)\n",
    "        markdown += formatted_answer\n",
    "        markdown_blocks.append(markdown)\n",
    "        \n",
    "    return '\\n\\n'.join(markdown_blocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 CSV to Markdown Conversion Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_csv_to_markdown(csv_path: Path, output_dir: Path = None) -> Optional[Path]:\n",
    "    \"\"\"Process a CSV file to Markdown Q&A format.\n",
    "    \n",
    "    Args:\n",
    "        csv_path: Path to the CSV file\n",
    "        output_dir: Directory to save markdown file (default: MD_DIR)\n",
    "        \n",
    "    Returns:\n",
    "        Optional[Path]: Path to the generated markdown file, or None if failed\n",
    "    \"\"\"\n",
    "    output_dir = output_dir or MD_DIR\n",
    "    logger.info(f\"Processing {csv_path} to markdown\")\n",
    "    \n",
    "    try:\n",
    "        # Read CSV without headers\n",
    "        df = pd.read_csv(csv_path, header=None)\n",
    "        \n",
    "        # Extract Q&A blocks\n",
    "        blocks = extract_blocks(df)\n",
    "        if not blocks:\n",
    "            logger.warning(f\"No content blocks found in {csv_path}\")\n",
    "            return None\n",
    "            \n",
    "        # Generate markdown\n",
    "        markdown_text = convert_blocks_to_markdown(blocks)\n",
    "        \n",
    "        # Save markdown file\n",
    "        base_name = csv_path.stem\n",
    "        output_path = output_dir / f'{base_name}.md'\n",
    "        save_markdown(markdown_text, output_path)\n",
    "        \n",
    "        logger.info(f\"Processed {csv_path.name} → {output_path.name} with {len(blocks)} Q&A blocks\")\n",
    "        return output_path\n",
    "        \n",
    "    except pd.errors.EmptyDataError:\n",
    "        logger.warning(f\"Empty or invalid CSV file: {csv_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing {csv_path}: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Batch Process CSVs to Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-04 15:46:02 - data_preparation - INFO - Starting batch conversion of CSVs to markdown\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Found 33 CSV files to process (excluding 3 files)\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Processing data/processing/csvs/NWA.csv to markdown\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Saved markdown file: data/processing/mds/NWA.md\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Processed NWA.csv → NWA.md with 5 Q&A blocks\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Processing data/processing/csvs/Jubilee_Life_.csv to markdown\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Saved markdown file: data/processing/mds/Jubilee_Life_.md\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Processed Jubilee_Life_.csv → Jubilee_Life_.md with 4 Q&A blocks\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Processing data/processing/csvs/VPBA.csv to markdown\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Saved markdown file: data/processing/mds/VPBA.md\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Processed VPBA.csv → VPBA.md with 7 Q&A blocks\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Processing data/processing/csvs/NSDA.csv to markdown\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Saved markdown file: data/processing/mds/NSDA.md\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Processed NSDA.csv → NSDA.md with 5 Q&A blocks\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Processing data/processing/csvs/EFU_Life.csv to markdown\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Saved markdown file: data/processing/mds/EFU_Life.md\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Processed EFU_Life.csv → EFU_Life.md with 4 Q&A blocks\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Processing data/processing/csvs/RDA.csv to markdown\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Saved markdown file: data/processing/mds/RDA.md\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Processed RDA.csv → RDA.md with 7 Q&A blocks\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Processing data/processing/csvs/LCA.csv to markdown\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Saved markdown file: data/processing/mds/LCA.md\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Processed LCA.csv → LCA.md with 6 Q&A blocks\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Processing data/processing/csvs/VPCA.csv to markdown\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Saved markdown file: data/processing/mds/VPCA.md\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Processed VPCA.csv → VPCA.md with 4 Q&A blocks\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Processing data/processing/csvs/ESFCA.csv to markdown\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Saved markdown file: data/processing/mds/ESFCA.md\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Processed ESFCA.csv → ESFCA.md with 5 Q&A blocks\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Processing data/processing/csvs/PF.csv to markdown\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Saved markdown file: data/processing/mds/PF.md\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Processed PF.csv → PF.md with 13 Q&A blocks\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Processing data/processing/csvs/NIF.csv to markdown\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Saved markdown file: data/processing/mds/NIF.md\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Processed NIF.csv → NIF.md with 18 Q&A blocks\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Processing data/processing/csvs/NFMF.csv to markdown\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Saved markdown file: data/processing/mds/NFMF.md\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Processed NFMF.csv → NFMF.md with 10 Q&A blocks\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Processing data/processing/csvs/HOME_REMITTANCE.csv to markdown\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Saved markdown file: data/processing/mds/HOME_REMITTANCE.md\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Processed HOME_REMITTANCE.csv → HOME_REMITTANCE.md with 20 Q&A blocks\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Processing data/processing/csvs/NHF.csv to markdown\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Saved markdown file: data/processing/mds/NHF.md\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Processed NHF.csv → NHF.md with 12 Q&A blocks\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Processing data/processing/csvs/PMYB__ALS.csv to markdown\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Saved markdown file: data/processing/mds/PMYB__ALS.md\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Processed PMYB__ALS.csv → PMYB__ALS.md with 14 Q&A blocks\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Processing data/processing/csvs/NFBF.csv to markdown\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Saved markdown file: data/processing/mds/NFBF.md\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Processed NFBF.csv → NFBF.md with 15 Q&A blocks\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Processing data/processing/csvs/PWRA.csv to markdown\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Saved markdown file: data/processing/mds/PWRA.md\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Processed PWRA.csv → PWRA.md with 7 Q&A blocks\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Processing data/processing/csvs/PLS.csv to markdown\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Saved markdown file: data/processing/mds/PLS.md\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Processed PLS.csv → PLS.md with 4 Q&A blocks\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Processing data/processing/csvs/NRF.csv to markdown\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Saved markdown file: data/processing/mds/NRF.md\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Processed NRF.csv → NRF.md with 10 Q&A blocks\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Processing data/processing/csvs/NADA.csv to markdown\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Saved markdown file: data/processing/mds/NADA.md\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Processed NADA.csv → NADA.md with 6 Q&A blocks\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Processing data/processing/csvs/NSA.csv to markdown\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Saved markdown file: data/processing/mds/NSA.md\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Processed NSA.csv → NSA.md with 14 Q&A blocks\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Processing data/processing/csvs/NSF.csv to markdown\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Saved markdown file: data/processing/mds/NSF.md\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Processed NSF.csv → NSF.md with 15 Q&A blocks\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Processing data/processing/csvs/NMF.csv to markdown\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Saved markdown file: data/processing/mds/NMF.md\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Processed NMF.csv → NMF.md with 20 Q&A blocks\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Processing data/processing/csvs/Nust_Life.csv to markdown\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Saved markdown file: data/processing/mds/Nust_Life.md\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Processed Nust_Life.csv → Nust_Life.md with 4 Q&A blocks\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Processing data/processing/csvs/VP-BA.csv to markdown\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Saved markdown file: data/processing/mds/VP-BA.md\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Processed VP-BA.csv → VP-BA.md with 7 Q&A blocks\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Processing data/processing/csvs/NUST4Car.csv to markdown\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Saved markdown file: data/processing/mds/NUST4Car.md\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Processed NUST4Car.csv → NUST4Car.md with 19 Q&A blocks\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Processing data/processing/csvs/NUF.csv to markdown\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Saved markdown file: data/processing/mds/NUF.md\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Processed NUF.csv → NUF.md with 14 Q&A blocks\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Processing data/processing/csvs/NADRA.csv to markdown\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Saved markdown file: data/processing/mds/NADRA.md\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Processed NADRA.csv → NADRA.md with 5 Q&A blocks\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Processing data/processing/csvs/NMA.csv to markdown\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Saved markdown file: data/processing/mds/NMA.md\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Processed NMA.csv → NMA.md with 5 Q&A blocks\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Processing data/processing/csvs/NMC.csv to markdown\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Saved markdown file: data/processing/mds/NMC.md\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Processed NMC.csv → NMC.md with 13 Q&A blocks\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Processing data/processing/csvs/CDA.csv to markdown\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Saved markdown file: data/processing/mds/CDA.md\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Processed CDA.csv → CDA.md with 4 Q&A blocks\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Processing data/processing/csvs/NAA.csv to markdown\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Saved markdown file: data/processing/mds/NAA.md\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Processed NAA.csv → NAA.md with 5 Q&A blocks\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Processing data/processing/csvs/NFDA.csv to markdown\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Saved markdown file: data/processing/mds/NFDA.md\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - Processed NFDA.csv → NFDA.md with 5 Q&A blocks\n",
      "2025-05-04 15:46:02 - data_preparation - INFO - CSV processing complete: 33 successes, 0 errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[✓] Processed 33 CSV files to markdown format\n"
     ]
    }
   ],
   "source": [
    "def batch_process_csvs_to_markdown() -> Tuple[int, int]:\n",
    "    \"\"\"Batch process all CSV files to markdown (excluding specified ones).\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[int, int]: (number of successful conversions, number of failures)\n",
    "    \"\"\"\n",
    "    logger.info(f\"Starting batch conversion of CSVs to markdown\")\n",
    "    \n",
    "    # Get list of all CSV files\n",
    "    excluded_files = set(CONFIG[\"excluded_csvs\"])\n",
    "    csv_files = [f for f in os.listdir(CSV_DIR) if f.endswith('.csv') and f not in excluded_files]\n",
    "    \n",
    "    logger.info(f\"Found {len(csv_files)} CSV files to process (excluding {len(excluded_files)} files)\")\n",
    "    \n",
    "    # Process each CSV file\n",
    "    success_count = 0\n",
    "    error_count = 0\n",
    "    \n",
    "    for csv_file in csv_files:\n",
    "        csv_path = CSV_DIR / csv_file\n",
    "        result = process_csv_to_markdown(csv_path)\n",
    "        \n",
    "        if result:\n",
    "            success_count += 1\n",
    "        else:\n",
    "            error_count += 1\n",
    "            \n",
    "    logger.info(f\"CSV processing complete: {success_count} successes, {error_count} errors\")\n",
    "    return success_count, error_count\n",
    "\n",
    "# Run the batch CSV to markdown conversion\n",
    "success_count, error_count = batch_process_csvs_to_markdown()\n",
    "print(f'[✓] Processed {success_count} CSV files to markdown format')\n",
    "if error_count > 0:\n",
    "    print(f'[!] {error_count} files had processing errors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Rate Sheet Markdown <a id='ratesheet'></a>\n",
    "---\n",
    "This section handles the extraction and formatting of rate sheet data into structured markdown tables.\n",
    "\n",
    "### 5.1 Rate Sheet Extraction Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_declaration(df: pd.DataFrame) -> str:\n",
    "    \"\"\"Extract declaration text from first few rows of CSV.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing the rate sheet data\n",
    "        \n",
    "    Returns:\n",
    "        str: Declaration text or empty string if not found\n",
    "    \"\"\"\n",
    "    for i in range(min(5, len(df))):\n",
    "        cell = str(df.iloc[i, 1]).strip() if pd.notna(df.iloc[i, 1]) else \"\"\n",
    "        if \"Indicative Profit Rates\" in cell:\n",
    "            return cell.replace(\"\\n\", \" \").strip('\"')\n",
    "    return \"\"\n",
    "\n",
    "def is_savings_account(cell: str) -> bool:\n",
    "    \"\"\"Check if cell contains a savings account name.\n",
    "    \n",
    "    Args:\n",
    "        cell: Cell content to check\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if cell likely contains a savings account name\n",
    "    \"\"\"\n",
    "    if not isinstance(cell, str):\n",
    "        return False\n",
    "        \n",
    "    cell = cell.strip().replace(\",\", \"\")\n",
    "    \n",
    "    if not cell or cell in [\"Profit Payment\", \"Profit Rate\", \"Tenor\"]:\n",
    "        return False\n",
    "        \n",
    "    return cell in KNOWN_SAVINGS_ACCOUNTS or cell.endswith(\"Account\")\n",
    "\n",
    "def tenor_sorter(tenor: str) -> int:\n",
    "    \"\"\"Sort tenors by duration for consistent order in markdown.\n",
    "    \n",
    "    Args:\n",
    "        tenor: Tenor string like \"3 Months\" or \"1 Year\"\n",
    "        \n",
    "    Returns:\n",
    "        int: Numeric value (in days) for sorting\n",
    "    \"\"\"\n",
    "    # Extract numeric value if present\n",
    "    num = re.search(r'(\\d+)', tenor)\n",
    "    if num:\n",
    "        value = int(num.group(1))\n",
    "        return {\n",
    "            \"Day\": value,\n",
    "            \"Month\": value * 30,\n",
    "            \"Year\": value * 365\n",
    "        }.get(next((unit for unit in [\"Day\", \"Month\", \"Year\"] if unit in tenor), \"\"), 9999)\n",
    "    \n",
    "    # Handle text-based tenors\n",
    "    for unit, prefixes in {\n",
    "        \"Month\": {\"One\": 30, \"Two\": 60, \"Three\": 90, \"Six\": 180},\n",
    "        \"Year\": {\"One\": 365, \"Two\": 730, \"Three\": 1095, \"Five\": 1825}\n",
    "    }.items():\n",
    "        if unit in tenor:\n",
    "            return next((days for prefix, days in prefixes.items() if prefix in tenor), 9999)\n",
    "    \n",
    "    return 9999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Rate Sheet Data Extraction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_savings_accounts(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Extract savings account information from dataframe.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing the rate sheet data\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with savings account data\n",
    "    \"\"\"\n",
    "    logger.info(\"Extracting savings account information\")\n",
    "    rows = []\n",
    "    current_account = None\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        # Look for account names\n",
    "        cell = str(row[1]).strip() if pd.notna(row[1]) else \"\"\n",
    "        if is_savings_account(cell):\n",
    "            current_account = cell\n",
    "            \n",
    "        # Look for profit payment rows\n",
    "        if isinstance(row[1], str) and \"Profit Payment\" in row[1]:\n",
    "            # Check next few rows for payment frequencies\n",
    "            for j in range(1, 5):\n",
    "                if i + j >= len(df):\n",
    "                    break\n",
    "                    \n",
    "                freq = df.iloc[i + j, 1]\n",
    "                \n",
    "                # Find rate in columns to the right\n",
    "                rate = next(\n",
    "                    (df.iloc[i + j, k] for k in range(2, 6) \n",
    "                     if k < len(df.columns) and pd.notna(df.iloc[i + j, k]) \n",
    "                     and re.match(r\"^\\d*\\.?\\d+$\", str(df.iloc[i + j, k]).strip())), \n",
    "                    None\n",
    "                )\n",
    "                \n",
    "                # Check if we have valid payment frequency\n",
    "                if (isinstance(freq, str) and \n",
    "                    re.match(r\"^(Monthly|Quarterly|Semi-Annually|Annually)\", freq.strip()) and \n",
    "                    current_account):\n",
    "                    rows.append({\n",
    "                        \"Account Name\": current_account,\n",
    "                        \"Profit Payment\": freq.strip(),\n",
    "                        \"Profit Rate\": float(rate) if rate else None,\n",
    "                    })\n",
    "    \n",
    "    logger.info(f\"Found {len(rows)} savings accounts\")\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def process_bachat_account(df: pd.DataFrame, i: int) -> List[Dict]:\n",
    "    \"\"\"Process NUST Bachat Account entries.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing the rate sheet data\n",
    "        i: Row index to start from\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: List of Bachat Account entry dictionaries\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    j = i + 2\n",
    "    \n",
    "    while j < len(df) and j < i + 5:\n",
    "        next_row = df.iloc[j].fillna(\"\").astype(str).tolist()\n",
    "        if len(next_row) > 8:\n",
    "            tenor = next_row[5].strip()\n",
    "            payout = next_row[6].strip()\n",
    "            rate = next_row[8].strip()\n",
    "            \n",
    "            if all([tenor, payout, rate]) and re.match(r\"^\\d*\\.?\\d+$\", rate):\n",
    "                rows.append({\n",
    "                    \"Account Name\": \"NUST Bachat Account\",\n",
    "                    \"Tenor\": tenor,\n",
    "                    \"Payout\": payout,\n",
    "                    \"Profit Rate\": float(rate),\n",
    "                })\n",
    "        j += 1\n",
    "        \n",
    "    return rows\n",
    "\n",
    "def get_account_name(df: pd.DataFrame, i: int, default_name: str) -> str:\n",
    "    \"\"\"Get account name for term deposits.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing the rate sheet data\n",
    "        i: Row index\n",
    "        default_name: Default name to use if better one isn't found\n",
    "        \n",
    "    Returns:\n",
    "        str: Account name\n",
    "    \"\"\"\n",
    "    account_name = default_name.strip()\n",
    "    if \"Deposit Receipt\" in default_name or \"Term Deposit\" in default_name:\n",
    "        return account_name\n",
    "    \n",
    "    account_name = \"Regular Term Deposit\"  # Default fallback\n",
    "    \n",
    "    # Check previous row for better name\n",
    "    if i > 0:\n",
    "        prev_row = df.iloc[i-1].fillna(\"\").astype(str).tolist()\n",
    "        if len(prev_row) > 5 and prev_row[5].strip():\n",
    "            account_name = prev_row[5].strip()\n",
    "            \n",
    "    return account_name\n",
    "\n",
    "def process_term_deposit_rows(df: pd.DataFrame, i: int) -> List[Dict]:\n",
    "    \"\"\"Process term deposit rows.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing the rate sheet data\n",
    "        i: Row index to start from\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: List of term deposit entry dictionaries\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    j = i + 2\n",
    "    \n",
    "    while j < len(df):\n",
    "        next_row = df.iloc[j].fillna(\"\").astype(str).tolist()\n",
    "        if len(next_row) <= 8:\n",
    "            j += 1\n",
    "            continue\n",
    "            \n",
    "        tenor = next_row[5].strip()\n",
    "        payout = next_row[6].strip()\n",
    "        rate = next_row[8].strip()\n",
    "        \n",
    "        # Check if we've reached the end of the deposit section\n",
    "        if not tenor or (not re.search(r\"\\d\", tenor) and \n",
    "                        not any(word in tenor for word in [\"One\", \"Two\", \"Three\", \"Six\", \"Five\"])):\n",
    "            break\n",
    "            \n",
    "        # Process valid rate entries\n",
    "        if re.match(r\"^\\d*\\.?\\d+$\", rate):\n",
    "            rows.append({\n",
    "                \"Account Name\": get_account_name(df, i, df.iloc[i][5]),\n",
    "                \"Tenor\": tenor,\n",
    "                \"Payout\": payout if payout else \"Maturity\",\n",
    "                \"Profit Rate\": float(rate),\n",
    "            })\n",
    "            \n",
    "        j += 1\n",
    "        \n",
    "    return rows\n",
    "\n",
    "def extract_term_deposits(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Extract term deposit information from dataframe.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing the rate sheet data\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with term deposit data\n",
    "    \"\"\"\n",
    "    logger.info(\"Extracting term deposit information\")\n",
    "    rows = []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        row = df.iloc[i].fillna(\"\").astype(str).tolist()\n",
    "        \n",
    "        if len(row) <= 5 or row[5] == \"\":\n",
    "            continue\n",
    "            \n",
    "        # Process NUST Bachat Account\n",
    "        if \"NUST Bachat Account\" in row[5]:\n",
    "            rows.extend(process_bachat_account(df, i))\n",
    "            continue\n",
    "\n",
    "        # Process Term Deposits\n",
    "        if \"Term Deposits\" in row[5] or any(term in row[5] for term in [\"Deposit Receipt\", \"Term Deposit\"]):\n",
    "            if i + 1 >= len(df):\n",
    "                continue\n",
    "                \n",
    "            next_row = df.iloc[i+1].fillna(\"\").astype(str).tolist()\n",
    "            if len(next_row) > 5 and \"Tenor\" in next_row[5]:\n",
    "                account_name = get_account_name(df, i, row[5])\n",
    "                \n",
    "                # Skip headers or empty sections\n",
    "                if any(phrase in account_name for phrase in [\"Change\", \"TERM DEPOSITS\"]):\n",
    "                    continue\n",
    "                \n",
    "                rows.extend(process_term_deposit_rows(df, i))\n",
    "\n",
    "    logger.info(f\"Found {len(rows)} term deposits\")\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def extract_fcy_rates(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Extract foreign currency rates from dataframe.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing the rate sheet data\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with FCY rate data\n",
    "    \"\"\"\n",
    "    logger.info(\"Extracting foreign currency (FCY) rates\")\n",
    "    fcy_data = []\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        row = df.iloc[i].fillna(\"\").astype(str).tolist()\n",
    "        if len(row) > 6 and \"FCY\" in row[5]:\n",
    "            for j in range(i+1, i+4):\n",
    "                if j < len(df):\n",
    "                    next_row = df.iloc[j].fillna(\"\").astype(str).tolist()\n",
    "                    if len(next_row) > 8:\n",
    "                        account_type = next_row[5].strip()\n",
    "                        if not account_type:\n",
    "                            continue\n",
    "                            \n",
    "                        usd, gbp, eur = map(str.strip, next_row[6:9])\n",
    "                        \n",
    "                        if re.match(r\"^\\d*\\.?\\d+$\", usd):\n",
    "                            fcy_data.append({\n",
    "                                \"Account Type\": account_type,\n",
    "                                \"USD\": float(usd) * 100,  # Convert to percentage\n",
    "                                \"GBP\": float(gbp) * 100,\n",
    "                                \"EUR\": float(eur) * 100\n",
    "                            })\n",
    "    \n",
    "    logger.info(f\"Found {len(fcy_data)} FCY rates\")\n",
    "    return pd.DataFrame(fcy_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Rate Sheet Markdown Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-04 15:47:12 - data_preparation - INFO - Processing rate sheet from data/processing/csvs/Rate_Sheet_July_1_2024.csv\n",
      "2025-05-04 15:47:12 - data_preparation - INFO - Extracting savings accounts...\n",
      "2025-05-04 15:47:12 - data_preparation - INFO - Extracting savings account information\n",
      "2025-05-04 15:47:12 - data_preparation - INFO - Found 12 savings accounts\n",
      "2025-05-04 15:47:12 - data_preparation - INFO - Extracting term deposits...\n",
      "2025-05-04 15:47:12 - data_preparation - INFO - Extracting term deposit information\n",
      "2025-05-04 15:47:12 - data_preparation - INFO - Found 23 term deposits\n",
      "2025-05-04 15:47:12 - data_preparation - INFO - Extracting FCY rates...\n",
      "2025-05-04 15:47:12 - data_preparation - INFO - Extracting foreign currency (FCY) rates\n",
      "2025-05-04 15:47:12 - data_preparation - INFO - Found 2 FCY rates\n",
      "2025-05-04 15:47:12 - data_preparation - INFO - Generating markdown output...\n",
      "2025-05-04 15:47:12 - data_preparation - INFO - Rate sheet markdown saved to data/processing/mds/Rate_Sheet.md\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[✓] Rate sheet markdown generated successfully\n"
     ]
    }
   ],
   "source": [
    "def generate_rate_sheet_markdown(csv_path: Path, output_path: Path = None) -> Optional[Path]:\n",
    "    \"\"\"Generate markdown from CSV rate sheet.\n",
    "    \n",
    "    Args:\n",
    "        csv_path: Path to rate sheet CSV\n",
    "        output_path: Path to save the markdown file (default: from CONFIG)\n",
    "        \n",
    "    Returns:\n",
    "        Optional[Path]: Path to the generated markdown file, or None if failed\n",
    "    \"\"\"\n",
    "    output_path = output_path or CONFIG[\"output_paths\"][\"rate_sheet_md\"]\n",
    "    logger.info(f\"Processing rate sheet from {csv_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Read CSV file\n",
    "        df = pd.read_csv(csv_path, header=None)\n",
    "        \n",
    "        # Extract data sections\n",
    "        declaration = extract_declaration(df)\n",
    "        logger.info(\"Extracting savings accounts...\")\n",
    "        savings_df = extract_savings_accounts(df)\n",
    "        \n",
    "        logger.info(\"Extracting term deposits...\")\n",
    "        term_deposits_df = extract_term_deposits(df)\n",
    "        \n",
    "        logger.info(\"Extracting FCY rates...\")\n",
    "        fcy_df = extract_fcy_rates(df)\n",
    "        \n",
    "        # Sort term deposits by tenor\n",
    "        if not term_deposits_df.empty:\n",
    "            term_deposits_df['SortOrder'] = term_deposits_df['Tenor'].apply(tenor_sorter)\n",
    "            term_deposits_df = term_deposits_df.sort_values(['Account Name', 'SortOrder'])\n",
    "            term_deposits_df = term_deposits_df.drop('SortOrder', axis=1)\n",
    "        \n",
    "        # Generate markdown\n",
    "        logger.info(\"Generating markdown output...\")\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            # Add disclaimer/declaration if available\n",
    "            if declaration:\n",
    "                f.write(f\"*{declaration}*\\n\\n\")\n",
    "            \n",
    "            # Add savings accounts section\n",
    "            f.write(\"## Savings Accounts\\n\\n\")\n",
    "            f.write(savings_df.to_markdown(index=False))\n",
    "            f.write(\"\\n\\n\")\n",
    "            \n",
    "            # Add term deposits section\n",
    "            f.write(\"## Term Deposit Accounts\\n\\n\")\n",
    "            f.write(term_deposits_df.to_markdown(index=False))\n",
    "            f.write(\"\\n\\n\")\n",
    "            \n",
    "            # Add FCY rates section if available\n",
    "            if not fcy_df.empty:\n",
    "                f.write(\"## Foreign Currency (FCY) Rates\\n\\n\")\n",
    "                fcy_df_formatted = fcy_df.copy()\n",
    "                for col in ['USD', 'GBP', 'EUR']:\n",
    "                    if col in fcy_df.columns:\n",
    "                        fcy_df_formatted[col] = fcy_df_formatted[col].apply(\n",
    "                            lambda x: f\"{x}%\" if isinstance(x, (int, float)) else x\n",
    "                        )\n",
    "                f.write(fcy_df_formatted.to_markdown(index=False))\n",
    "                f.write(\"\\n\\n\")\n",
    "        \n",
    "        logger.info(f\"Rate sheet markdown saved to {output_path}\")\n",
    "        return output_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing rate sheet {csv_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Process rate sheet to markdown\n",
    "rate_sheet_path = generate_rate_sheet_markdown(CONFIG[\"file_paths\"][\"rate_sheet_csv\"])\n",
    "if rate_sheet_path:\n",
    "    print(f'[✓] Rate sheet markdown generated successfully')\n",
    "else:\n",
    "    print(f'[!] Failed to generate rate sheet markdown')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Product List Markdown <a id='productlist'></a>\n",
    "---\n",
    "This section combines all the generated markdown files into a single comprehensive document.\n",
    "\n",
    "### 6.1 Markdown Combination Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-04 15:47:54 - data_preparation - INFO - Merging markdown files from data/processing/mds to data/processing/unreviewed_RAG.md\n",
      "2025-05-04 15:47:54 - data_preparation - INFO - Combined markdown with 33 products saved to data/processing/unreviewed_RAG.md\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[✓] Combined markdown document generated: data/processing/unreviewed_RAG.md\n"
     ]
    }
   ],
   "source": [
    "def merge_markdown_files(output_path: Path = None) -> Optional[Path]:\n",
    "    \"\"\"Merge all markdown files into a single comprehensive document.\n",
    "    \n",
    "    Args:\n",
    "        output_path: Path to save the combined markdown (default: from CONFIG)\n",
    "        \n",
    "    Returns:\n",
    "        Optional[Path]: Path to the combined markdown file, or None if failed\n",
    "    \"\"\"\n",
    "    output_path = output_path or CONFIG[\"output_paths\"][\"combined_md\"]\n",
    "    dir_path = MD_DIR\n",
    "    \n",
    "    logger.info(f\"Merging markdown files from {dir_path} to {output_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Ensure output directory exists\n",
    "        output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        with open(output_path, 'w', encoding='utf-8') as outfile:\n",
    "            # Write overview section\n",
    "            outfile.write(\"# NUST Bank Products Overview\\n\\n\")\n",
    "            \n",
    "            # Write product categories in table of contents\n",
    "            for category, files in PRODUCTS.items():\n",
    "                outfile.write(f\"## {category} Products\\n\\n\")\n",
    "                for _, new_name in files:\n",
    "                    outfile.write(f\"- {new_name.replace('.md', '')}\\n\")\n",
    "                outfile.write(\"\\n\")\n",
    "\n",
    "            # Write FAQ section first\n",
    "            outfile.write(\"\\n# FAQ\\n\\n\")\n",
    "            outfile.write(read_file(dir_path / 'faq.md'))\n",
    "            outfile.write(\"\\n\\n\")\n",
    "\n",
    "            # Write Rate Sheet section second\n",
    "            outfile.write(\"\\n# Rate Sheet\\n\\n\")\n",
    "            outfile.write(read_file(dir_path / 'Rate_Sheet.md'))\n",
    "            outfile.write(\"\\n\\n\")\n",
    "\n",
    "            # Write detailed product sections\n",
    "            outfile.write(\"\\n# Detailed Product Information\\n\\n\")\n",
    "            \n",
    "            # Process each category\n",
    "            product_count = 0\n",
    "            for category, files in PRODUCTS.items():\n",
    "                outfile.write(f\"## {category} Products\\n\\n\")\n",
    "                \n",
    "                for old_name, new_name in files:\n",
    "                    # Skip rate sheet as it's already included\n",
    "                    if old_name != 'Rate_Sheet.md':\n",
    "                        outfile.write(f\"### {new_name.replace('.md', '')}\\n\\n\")\n",
    "                        content = read_file(dir_path / old_name)\n",
    "                        outfile.write(content)\n",
    "                        outfile.write(\"\\n\\n\")\n",
    "                        product_count += 1\n",
    "        \n",
    "        logger.info(f\"Combined markdown with {product_count} products saved to {output_path}\")\n",
    "        return output_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error merging markdown files: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Merge all markdown files\n",
    "combined_path = merge_markdown_files()\n",
    "if combined_path:\n",
    "    print(f'[✓] Combined markdown document generated: {combined_path}')\n",
    "else:\n",
    "    print(f'[!] Failed to generate combined markdown document')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary & Next Steps <a id='summary'></a>\n",
    "---\n",
    "This section summarizes the data processing pipeline and provides guidance for manual review and next steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Manual Review Notes\n",
    "\n",
    "The generated markdown document (`unreviewed_RAG.md`) requires manual review for optimal formatting. Manually reviewing our dataset makes it high-value, more accurate, and trustworthy as opposed to using a LLM.\n",
    "\n",
    "Here are the specific items to review:\n",
    " \n",
    "1. **Extra Bullet Points**: Some markdown files may contain redundant bullet points, particularly when the original CSV had ambiguous list formatting.\n",
    " \n",
    "2. **Inconsistent Headers**: Check for proper heading level hierarchy throughout the document.\n",
    " \n",
    "3. **Table Formatting**: Ensure all tables in the Rate Sheet section are properly aligned and readable.\n",
    " \n",
    "4. **Image References**: If any markdown refers to images, ensure the paths are correct or the references are removed.\n",
    " \n",
    "5. **Duplicate Content**: Some products may have overlapping information that should be consolidated.\n",
    " \n",
    "After manual review and editing, save the final version to `data/processed/RAG.md` for use in downstream pipelines.\n",
    " \n",
    "📝 NEXT STEPS:\n",
    "1. Manually review the 'data/processing/unreviewed_RAG.md' file\n",
    "2. Fix any formatting issues (e.g., extra bullets, inconsistent headers)\n",
    "3. Save final version to 'data/processed/RAG.md'\n",
    "4. Use the final document for downstream RAG processing\n",
    "\n",
    "⚠️ KNOWN FORMATTING ISSUES TO CHECK:\n",
    "- Extra bullet points\n",
    "- Redundant bullets within bullets\n",
    "- Bullets using \"·\" instead of \"*\"\n",
    "- Inconsistent spacing between sections\n",
    "- Table formattings\n",
    "- Missing or malformed Q/A pairs\n",
    "- Junk instances of \"Main\" in the markdown\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
